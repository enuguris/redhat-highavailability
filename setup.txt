RedHat HighAvailibility Cluster

#Preparing a custom image for use in Azure.
https://docs.microsoft.com/en-us/azure/virtual-machines/linux/redhat-create-upload-vhd

#Reference to add hv drivers
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/pdf/deploying_red_hat_enterprise_linux_8_on_public_cloud_platforms/Red_Hat_Enterprise_Linux-8-Deploying_Red_Hat_Enterprise_Linux_8_on_public_cloud_platforms-en-US.pdf

#Add Hypervisor drivers
cat > /etc/dracut.conf.d/hv.conf <<EOF
add_drivers+=" hv_vmbus "
add_drivers+=" hv_netvsc "
add_drivers+=" hv_storvsc "
EOF

#regenerate initrd image 
dracut -f -v --regenerate-all


#To install WALinuxAgent enable AppStream repo
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms

#Install these packages
yum install -y WALinuxAgent cloud-init cloud-utils-growpart gdisk hyperv-daemons

#Enable waagent and cloud-init service
systemctl enable waagent.service
systemctl enable cloud-init.service

#Configure cloud-init to handle the provisioning

#Configure waagent for cloud-init
sed -i 's/Provisioning.Agent=auto/Provisioning.Agent=cloud-init/g' /etc/waagent.conf
sed -i 's/ResourceDisk.Format=y/ResourceDisk.Format=n/g' /etc/waagent.conf
sed -i 's/ResourceDisk.EnableSwap=y/ResourceDisk.EnableSwap=n/g' /etc/waagent.conf

#configure mounts and disk_setup at init stage
echo "Adding mounts and disk_setup to init stage"
sed -i '/ - mounts/d' /etc/cloud/cloud.cfg
sed -i '/ - disk_setup/d' /etc/cloud/cloud.cfg
sed -i '/cloud_init_modules/a\\ - mounts' /etc/cloud/cloud.cfg
sed -i '/cloud_init_modules/a\\ - disk_setup' /etc/cloud/cloud.cfg

#Configure Azure data source
echo "Allow only Azure datasource, disable fetching network setting via IMDS"
cat > /etc/cloud/cloud.cfg.d/91-azure_datasource.cfg <<EOF
datasource_list: [ Azure ]
datasource:
    Azure:
        apply_network_config: False
EOF

#if configured, remove swap file
if [[ -f /mnt/resource/swapfile ]]; then
echo "Removing swapfile" #RHEL uses a swapfile by defaul
swapoff /mnt/resource/swapfile
rm /mnt/resource/swapfile -f
fi

#Configure cloud-init logging:
echo "Add console log file"
cat >> /etc/cloud/cloud.cfg.d/05_logging.cfg <<EOF

# This tells cloud-init to redirect its stdout and stderr to
# 'tee -a /var/log/cloud-init-output.log' so the user can see output
# there without needing to look on the console.
output: {all: '| tee -a /var/log/cloud-init-output.log'}
EOF

#If you want mount, format and create swap you can either:
cat > /etc/cloud/cloud.cfg.d/00-azure-swap.cfg << EOF
#cloud-config
# Generated by Azure cloud image build
disk_setup:
  ephemeral0:
    table_type: mbr
    layout: [66, [33, 82]]
    overwrite: True
fs_setup:
  - device: ephemeral0.1
    filesystem: ext4
  - device: ephemeral0.2
    filesystem: swap
mounts:
  - ["ephemeral0.1", "/mnt"]
  - ["ephemeral0.2", "none", "swap", "sw", "0", "0"]
EOF

#Unregister the subscription before deprovisioning the virtual machines
subscription-manager unregister

#


Azure VM

VM Size: D2s_v3 (vm should support premium disks addition)
Shared Disk: Size 1TB, enable share disk during creation, use type of p30

OS: RHEL 8
ADDITIONAL REPOS: HighAvailibility 

#To register the system to redhat
subscription-manager register --auto-attach

#To check the subscription status
subscription-manager status

output:
[root@localhost ~]# subscription-manager status
+-------------------------------------------+
   System Status Details
+-------------------------------------------+
Overall Status: Current

#Enable highavailability-rpms repo on all nodes
subscription-manager repos --enable=rhel-8-for-x86_64-highavailability-rpms

#Disable firewalld and selinux on all nodes
systemctl disable --now firewalld
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config

#Install these HA packages on all nodes
dnf install pcs pacemaker fence-agents-all -y

#Enable pcs service on all nodes
systemctl enable pcsd.service --now

#Set passwd for hacluster user on all nodes
passwd hacluster

Example output:
[root@rhel8-ha-node1 ~]# passwd hacluster
Changing password for user hacluster.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.


[root@rhel8-ha-node2 ~]# passwd hacluster
Changing password for user hacluster.
New password:
BAD PASSWORD: The password is shorter than 8 characters
Retype new password:
passwd: all authentication tokens updated successfully.


#On any one of the cluster node, use pcs host auth to authenticate as the hacluster user. Use the below syntax:
#This step is only performed on one node. In this case rhel8-ha-node1
pcs host auth rhel8-ha-node1 rhel8-ha-node2

Example output:
[root@rhel8-ha-node1 ~]# pcs host auth rhel8-ha-node1 rhel8-ha-node2
Username: hacluster
Password:
rhel8-ha-node2: Authorized
rhel8-ha-node1: Authorized

#Create the two node cluster
#This step is only performed on one node. In this case rhel8-ha-node1
pcs cluster setup mq_cluster rhel8-ha-node1 rhel8-ha-node2

NOTE: Used --force as it was complaining a cluster config already existed. Need to look later about it.
NOTE: Using --force will destroy any other cluster config and will create a new one.

Example output:
[root@rhel8-ha-node1 ~]# pcs cluster setup mq_cluster rhel8-ha-node1 rhel8-ha-node2 --force
No addresses specified for host 'rhel8-ha-node1', using 'rhel8-ha-node1'
No addresses specified for host 'rhel8-ha-node2', using 'rhel8-ha-node2'
Warning: rhel8-ha-node2: Cluster configuration files found, the host seems to be in a cluster already
Warning: rhel8-ha-node1: Cluster configuration files found, the host seems to be in a cluster already
Destroying cluster on hosts: 'rhel8-ha-node1', 'rhel8-ha-node2'...
rhel8-ha-node1: Successfully destroyed cluster
rhel8-ha-node2: Successfully destroyed cluster
Requesting remove 'pcsd settings' from 'rhel8-ha-node1', 'rhel8-ha-node2'
rhel8-ha-node1: successful removal of the file 'pcsd settings'
rhel8-ha-node2: successful removal of the file 'pcsd settings'
Sending 'corosync authkey', 'pacemaker authkey' to 'rhel8-ha-node1', 'rhel8-ha-node2'
rhel8-ha-node1: successful distribution of the file 'corosync authkey'
rhel8-ha-node1: successful distribution of the file 'pacemaker authkey'
rhel8-ha-node2: successful distribution of the file 'corosync authkey'
rhel8-ha-node2: successful distribution of the file 'pacemaker authkey'
Sending 'corosync.conf' to 'rhel8-ha-node1', 'rhel8-ha-node2'
rhel8-ha-node1: successful distribution of the file 'corosync.conf'
rhel8-ha-node2: successful distribution of the file 'corosync.conf'
Cluster has been successfully set up.

#Start the cluster
#This step is only performed on one node. In this case rhel8-ha-node1
pcs cluster start --all

Output:
[root@rhel8-ha-node1 ~]# pcs cluster start --all
rhel8-ha-node1: Starting Cluster...
rhel8-ha-node2: Starting Cluster...

#Verify corosync installation
corosync-cfgtool -s

Output:
[root@rhel8-ha-node1 ~]# corosync-cfgtool -s
Printing link status.
Local node ID 1
LINK ID 0
        addr    = 10.1.0.5
        status:
                nodeid  1:      localhost
                nodeid  2:      connected

#check the membership and quorum APIs:
corosync-cmapctl | grep members

Output:
[root@rhel8-ha-node1 ~]# corosync-cmapctl | grep members
runtime.members.1.config_version (u64) = 0
runtime.members.1.ip (str) = r(0) ip(10.1.0.5)
runtime.members.1.join_count (u32) = 1
runtime.members.1.status (str) = joined
runtime.members.2.config_version (u64) = 0
runtime.members.2.ip (str) = r(0) ip(10.1.0.6)
runtime.members.2.join_count (u32) = 1
runtime.members.2.status (str) = joined

#Check the status of corosync across cluster nodes
pcs status corosync

Output:
[root@rhel8-ha-node1 ~]# pcs status corosync

Membership information
----------------------
    Nodeid      Votes Name
         1          1 rhel8-ha-node1 (local)
         2          1 rhel8-ha-node2

#check cluster status
pcs status

output:
[root@rhel8-ha-node1 ~]# pcs status
Cluster name: mq_cluster

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: rhel8-ha-node1 (version 2.0.5-9.el8_4.1-ba59be7122) - partition with quorum
  * Last updated: Thu Jun 10 08:20:11 2021
  * Last change:  Thu Jun 10 08:01:49 2021 by hacluster via crmd on rhel8-ha-node1
  * 2 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ rhel8-ha-node1 rhel8-ha-node2 ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/disabled
  pacemaker: active/disabled
  pcsd: active/enabled


#Enable corosync and pacemaker service to automatically start on boot on all the Linux HA Cluster nodes
systemctl enable --now pacemaker
systemctl enable --now corosync

#For the initial setup disable fencing, returns empty output
#To be performed only on one node
pcs property set stonith-enabled=false

#Configure active/passive Linux HA Cluster using a LVM resource

#Change system_id_source = "none" to system_id_source = "uname" in /etc/lvm/lvm.conf 
#on all cluster nodes

#Verify that the LVM system ID on the node matches the uname for the node.
lvm systemid

Output:
[root@rhel8-ha-node1 ~]# lvm systemid
  system ID: rhel8-ha-node1

[root@rhel8-ha-node2 ~]# lvm systemid
  system ID: rhel8-ha-node2

#Rebuild initramfs on all the cluster nodes with the following steps. Take backup of current initramfs file on all of the cluster nodes.
NOTE: FIND OUT WHY THIS STEP IS REQUIRED

cp /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.$(date +%m-%d-%H%M%S).bak
dracut -f -v

#Reboot all nodes
reboot

#Create logical volume on shared storage
pvcreate /dev/sdc

#Create volume group 
vgcreate cluster_vg /dev/sdc

#Verify that the new volume group has the system ID of the node on which you are running and from which you created the volume group.
vgs -o+systemid

#Create logical volume
lvcreate -l 100%FREE -n cluster_lv cluster_vg

#Create a filesystem of your requirement (ext4/xfs) over the newly created LVM device.
mkfs.xfs /dev/cluster_vg/cluster_lv

#Create mount points for your HA Logical Volumes on all the cluster nodes
mkdir /var/www

#Create a cluster resource with resource agent ocf:heartbeat:LVM-activate so the VG can be managed by Cluster.
pcs resource create my-vg ocf:heartbeat:LVM-activate vgname=cluster_vg activation_mode=exclusive vg_access_mode=system_id --group HA-LVM

#Create a cluster resource with resource agent ocf:heartbeat:Filesystem so cluster will control the mount of filesystem 
#It will make it available on one of the cluster node.
pcs resource create my-fs ocf:heartbeat:Filesystem device=/dev/cluster_vg/cluster_lv directory=/var/www fstype=xfs --group HA-LVM

#Verify cluster configuration
[root@rhel8-ha-node1 ~]# pcs status
Cluster name: mq_cluster
Cluster Summary:
  * Stack: corosync
  * Current DC: rhel8-ha-node1 (version 2.0.5-9.el8_4.1-ba59be7122) - partition with quorum
  * Last updated: Thu Jun 10 08:41:50 2021
  * Last change:  Thu Jun 10 08:40:59 2021 by root via cibadmin on rhel8-ha-node1
  * 2 nodes configured
  * 2 resource instances configured

Node List:
  * Online: [ rhel8-ha-node1 rhel8-ha-node2 ]

Full List of Resources:
  * Resource Group: HA-LVM:
    * my-vg     (ocf::heartbeat:LVM-activate):   Started rhel8-ha-node1
    * my-fs     (ocf::heartbeat:Filesystem):     Started rhel8-ha-node1

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled


#To test failover set the rhel8-ha-node1 to standby
pcs node standby rhel8-ha-node1


https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-http-server-in-a-cluster-configuring-and-managing-high-availability-clusters

#Install httpd and wget on both the nodes
yum install -y httpd wget

#Add lines to the file /etc/httpd/conf/httpd.conf
cat >> /etc/httpd/conf/httpd.conf << EOF
<Location /server-status>
    SetHandler server-status
    Require local
</Location>
EOF

#Remove the following line in the /etc/logrotate.d/httpd file on each node in the cluster.
/bin/systemctl reload httpd.service > /dev/null 2>/dev/null || true

#Add these lines in that place
/usr/bin/test -f /run/httpd.pid >/dev/null 2>/dev/null &&
/usr/bin/ps -q $(/usr/bin/cat /run/httpd.pid) >/dev/null 2>/dev/null &&
/usr/sbin/httpd -f /etc/httpd/conf/httpd.conf \
-c "PidFile /run/httpd.pid" -k graceful > /dev/null 2>/dev/null || true

#Create resource for floating ip
#NOTE: Make sure the ip here is the one used when creating an internal load balancer on Azure
pcs resource create ClusterIP ocf:heartbeat:IPaddr2 ip=10.1.0.10 cidr_netmask=24 nic=eth0 op monitor interval=30s --group HA-LVM

#Create resource for monitoring server-status
pcs resource create WebSite ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" --group HA-LVM

#Create Azure loadbalancer resource
pcs resource create p_azure-lb ocf:heartbeat:azure-lb op monitor OCF_CHECK_LEVEL="0" timeout="20s" interval="10s" --group HA-LVM

#Verify cluster status, try to failover and failback
#Check the /var/log/httpd/access_log on both nodes to see status checks entries

Output:
[root@rhel8-ha-node2 ~]# pcs status
Cluster name: mq_cluster
Cluster Summary:
  * Stack: corosync
  * Current DC: rhel8-ha-node1 (version 2.0.5-9.el8_4.1-ba59be7122) - partition with quorum
  * Last updated: Thu Jun 10 09:35:45 2021
  * Last change:  Thu Jun 10 09:35:42 2021 by root via cibadmin on rhel8-ha-node2
  * 2 nodes configured
  * 4 resource instances configured

Node List:
  * Online: [ rhel8-ha-node1 rhel8-ha-node2 ]

Full List of Resources:
  * Resource Group: HA-LVM:
    * my-vg     (ocf::heartbeat:LVM-activate):   Started rhel8-ha-node1
    * my-fs     (ocf::heartbeat:Filesystem):     Started rhel8-ha-node1
    * ClusterIP (ocf::heartbeat:IPaddr2):        Started rhel8-ha-node1
    * WebSite   (ocf::heartbeat:apache):         Started rhel8-ha-node1

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

References to fencing and wathdog:
https://access.redhat.com/solutions/3892631
https://access.redhat.com/solutions/65187  
  
#Configure scsi based fencing

#Install watchdog package on all cluster nodes
yum -y install watchdog

#For a hard reboot, link the script with the following command on all cluster nodes
ln -s /usr/share/cluster/fence_scsi_check /etc/watchdog.d/fence_scsi_check_hardreboot

#Add softdog module to auto load during boot
echo softdog > /etc/modules-load.d/watchdog.conf
systemctl restart systemd-modules-load

#Enable the watchdog service on all cluster nodes
systemctl enable --now watchdog

NOTE: ALWAYS USE THE DEVICE NAMES BASED BY ID TO AVOID ISSUES WITH RENAMING DISK NAME OF SHARED DISK ON CLUSTER NODES
NOTE: SHARED DISK NAME OF NODE1 CAN BE /dev/sdc BUT ON NODE2 IT CAN BE /dev/sdb

#List scsi devices with size info
lsscsi -s

#Determine the shared disk name based on above output. Get the shared disk device id. 
#replace X in /dev/sdX with the proper value
ls -lth /dev/disk/by-id/ | grep /dev/sdX

#Create stonith resource config, replace the disk's scsi id, just run on one node
pcs stonith create scsi fence_scsi pcmk_host_list="rhel8-ha-node1 rhel8-ha-node2" pcmk_reboot_action="on" devices="/dev/disk/by-id/scsi-360022480c9e5043a911161d43251ab21" meta provides="unfencing" --force

#Enable fencing, just run on one node
pcs property set stonith-enabled=true

#verity if is set fencing is set to true or not:
pcs property show stonith-enabled

Output:
[root@rhel8-ha-node1 ~]# pcs property show stonith-enabled
Cluster Properties:
 stonith-enabled: true

############TESTING FENCING###########

#fence the node on which service is active to test the failover, in this case rhel8-ha-node1 is the active host
stonith_admin -F rhel8-ha-node1

#Cluster service will start the services on node2 

CONFIGURING STORAGE-BASED FENCE DEVICES WITH UNFENCING
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/configuring_the_red_hat_high_availability_add-on_with_pacemaker/s1-unfence-haar

Commands to check scsi persistent reservations:

sg_persist binary is provided by the sg3_utils package

#To read full status of the reservation on the scsi device
[root@rhel8-ha-node1 ~]# sg_persist -d /dev/sdc -s
  Msft      Virtual Disk      1.0
  Peripheral device type: disk
  PR generation=0xe
    Key=0x811f0000
      All target ports bit clear
      Relative port address: 0x0
      not reservation holder
      Transport Id of initiator:
        SAS address: 0xad4be0371101140
    Key=0x811f0001
      All target ports bit clear
      Relative port address: 0x0
      << Reservation holder >>
      scope: LU_SCOPE,  type: obsolete [0]
      Transport Id of initiator:
        SAS address: 0x246df865861d5440

#To Lists all the reservation keys registered
[root@rhel8-ha-node1 ~]# sg_persist -d /dev/sdc -k
  Msft      Virtual Disk      1.0
  Peripheral device type: disk
  PR generation=0xe, 2 registered reservation keys follow:
    0x811f0000
    0x811f0001

#List  information  about  the  current
#holder  of  the  reservation  on  the  DEVICE
[root@rhel8-ha-node1 ~]# sg_persist -d /dev/sdc -r
  Msft      Virtual Disk      1.0
  Peripheral device type: disk
  PR generation=0xe, Reservation follows:
    Key=0x811f0001
    scope: LU_SCOPE,  type: Write Exclusive, registrants only
	
	
#Show stonith config
[root@rhel8-ha-node2 ~]# pcs stonith config
 Resource: scsi (class=stonith type=fence_scsi)
  Attributes: devices=/dev/sdc pcmk_host_list="rhel8-ha-node1 rhel8-ha-node2" pcmk_reboot_action=off
  Meta Attrs: provides=unfencing
  Operations: monitor interval=60s (scsi-monitor-interval-60s)
  
#Show resource group config
[root@rhel8-ha-node2 ~]# pcs resource config
 Group: HA-LVM
  Resource: my-vg (class=ocf provider=heartbeat type=LVM-activate)
   Attributes: activation_mode=exclusive vg_access_mode=system_id vgname=cluster_vg
   Operations: monitor interval=30s timeout=90s (my-vg-monitor-interval-30s)
               start interval=0s timeout=90s (my-vg-start-interval-0s)
               stop interval=0s timeout=90s (my-vg-stop-interval-0s)
  Resource: my-fs (class=ocf provider=heartbeat type=Filesystem)
   Attributes: device=/dev/cluster_vg/cluster_lv directory=/var/www/ fstype=xfs
   Operations: monitor interval=20s timeout=40s (my-fs-monitor-interval-20s)
               start interval=0s timeout=60s (my-fs-start-interval-0s)
               stop interval=0s timeout=60s (my-fs-stop-interval-0s)
  Resource: Website (class=ocf provider=heartbeat type=apache)
   Attributes: configfile=/etc/httpd/conf/httpd.conf statusurl=http://127.0.0.1/server-status
   Operations: monitor interval=10s timeout=20s (Website-monitor-interval-10s)
               start interval=0s timeout=40s (Website-start-interval-0s)
               stop interval=0s timeout=60s (Website-stop-interval-0s)


#Show pcs status
[root@rhel8-ha-node2 ~]# pcs status
Cluster name: devhacluster
Stack: corosync
Current DC: rhel8-ha-node2 (version 2.0.2-3.el8_1.2-744a30d655) - partition with quorum
Last updated: Wed Jun  9 11:28:37 2021
Last change: Wed Jun  9 11:17:49 2021 by root via cibadmin on rhel8-ha-node1

2 nodes configured
4 resources configured

Online: [ rhel8-ha-node1 rhel8-ha-node2 ]

Full list of resources:

 Resource Group: HA-LVM
     my-vg      (ocf::heartbeat:LVM-activate):  Started rhel8-ha-node2
     my-fs      (ocf::heartbeat:Filesystem):    Started rhel8-ha-node2
     Website    (ocf::heartbeat:apache):        Started rhel8-ha-node2
 scsi   (stonith:fence_scsi):   Started rhel8-ha-node1

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
